We present a system for reliably comparing BCI pipelines that is both easily
extended to incorporate new datasets and equipped with an automated statistical
procedure for determining which pipelines perform best. Furthermore, this system
defines a simple interface for submitting and validating new BCI pipelines,
which could serve to unify the many methods that exist so far. To test that
system, we present results using standard pipelines in contexts that have wide
relevance to the BCI community. By looking across multiple, large datasets, it is possible to make statements about how BCIs perform on average, without any sort of expert tuning of the processing chain, and further to see where the major pitfalls still lie.

The results of this analysis suggest that sample size and discrepancies in imagery and hardware have had a strong impact on the reproducibility of many BCI algorithms. Although various regularized approaches have been validated separately to outperform the original CSP algorithm, the result here is that none of them appear to make a difference. The only algorithm that reliably out-performs the other methods is the tangent space method, but even then it is not an enormous difference. As has been known for years, the majority of variability within a BCI paradigm is not due to the algorithm, but rather the user. People who do poorly do poorly nearly regardless of the processing used on them (with some exceptions), and those that do well can often manage a simple classification task with even the simplest pipeline.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
