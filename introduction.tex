Brain-computer interfaces (BCIs) have long presented the neuroscience methods
community with a unique challenge. Unlike fields like vision research, where one
simply has a database of images and labels, a BCI is defined by a signal
recorded from the brain and fed into a computer, which can be influenced in any
number of ways both by the subject and by the experimenter. As a result,
validating approaches has always been a difficult task. Number of channels,
requested task, physical setup, and many other features vary between the
numerous publically available datasets online, not to mention issues of
convenience such as file format and documentation. Because of this, the BCI
methods community has long done one of two things to validate an new approach:
Recorded a new dataset, or used one of few tried-and-true datasets that exist in
the wild.

Recording a new dataset, though an elegant way to show that a proposed method
works online, presents problems for post-hoc analysis. Without making code and
data public, it is impossible to know whether offline classification results are
convincing or due to some coding issue or recording artifact. Further, it is
well-known that differences in hardware (CITE), paradigm (CITE), and
subject(CITE) can have large differences in the outcome of a BCI task, making it
very difficult to generalize from any single dataset. Lastly, and possibly most
problematically, this puts the onus on each individual lab to reproduce the code
for all other competing methods in order to make a claim to be comparable with
the 'state-of-the-art.' As a result, the vast majority of novel BCI algorithm
papers compare either against other work from the same lab, or old standards
such as CSP \cite{Lotte2011}, with or without regularization, and LDA, or simple
channel-level variances combined with a classifier of choice. 

Using some of the standard datasets in the field, however, has its own
problems. Only in the last year and a half, over a thousand journal and
conference submissions have been written on the BCI Competition III
\cite{Blankertz2006,Schloegl2005} and IV \cite{Tangermann2012} datasets, using
selections to validate pipelines for processing EEG data into commands. These
datasets have been available publically for over a decade, implying that the
true number of papers which validate results using this data is simply
astronomical. As a result, and also because even these datasets are never used
in their entirity but rather piecemeal, the possibility of overfitting, or of
differences due to mis-coded competitor algorithms, is extremely high.

Much as Kaggle (CITE) has unified competition in some other areas of machine
learning, a platform for standardizing algorithm performance and competition is
required for BCIs as well. In contrast to other more standard machine learning
tasks, however, distinctions must be made for the many different possible
algorithms and metrics when working in BCIs. We propose our platform, the MOABB
(Mother Of All BCI Benchmarks) Project, as a candidate for this application. 

As an initial validation of this project, we present results on the constrained
task of binary classification in left or right hand imagined motor imagery, as
that is the most widely used motor imagery paradigm and allows us to exhibit the
process across the largest number of datasets. However, the format allows for
many other questions, including different channel types (EEG, fNIRS, or other),
multi-class paradigms, and also transfer learning scenarios as described in
\cite{Jayaram2016}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
