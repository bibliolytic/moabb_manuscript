Brain-computer interfaces (BCIs) have long presented the neuroscience methods
community with a unique challenge. Unlike fields like vision research, where one
simply has a database of images and labels, a BCI is defined by a signal
recorded from the brain and fed into a computer, which can be influenced in any
number of ways both by the subject and by the experimenter. As a result,
validating approaches has always been a difficult task. Number of channels,
requested task, physical setup, and many other features vary between the
numerous publically available datasets online, not to mention issues of
convenience such as file format and documentation. Because of this, the BCI
methods community has long done one of two things to validate an new approach:
Recorded a new dataset, or used one of few tried-and-true datasets that exist in
the wild.

Recording a new dataset, though an elegant way to show that a proposed method
works online, presents problems for post-hoc analysis. Without making 
data public, it is impossible to know whether offline classification results are
convincing or due to some coding issue or recording artifact. Further, it is
well-known that differences in hardware (CITE), paradigm (CITE), and
subject(CITE) can have large differences in the outcome of a BCI task, making it
very difficult to generalize findings from any single dataset.

When recording a new dataset is not an option, One can turn to some of the standard EEG datasets publicly available. Over the course of the last year and a half, over a thousand journal and
conference submissions have been written on the BCI Competition III
\cite{Blankertz2006,Schloegl2005} and IV \cite{Tangermann2012} datasets. Considering that
these datasets have been available publically for over a decade,  the
true number of papers which validate results against them is simply
astronomical. While it is impossible to argue about the impact those two datasets had on the field,
we must admit that relying so heavily on two dataset composed of a handful of subject each expose us to several issues (such as over-fitting and false positive).

(Moving this here, last 2 paragraph are about datasets)
Lastly, and possibly most
problematically, the scarcity of available code for newly published BCI algorithm puts the onus on each individual lab to reproduce the code
for all other competing methods in order to make a claim to be comparable with
the 'state-of-the-art' (SOA). As a result, the vast majority of novel BCI algorithm
papers compare either against other work from the same lab, or old standards
such as CSP \cite{Lotte2011} (we should cite the original CSP paper), with or without regularization, and LDA, or simple
channel-level variances combined with a classifier of choice (CITE). 

Theses problems have long been resolved in some other areas of machine
learning, such as computer vision or natural language processing, where a large
collection of unified datasets and up-to-date SOA algorithm implementation are available to 
researchers trough various software package (CITE one).
Hopefully, a large amount of EEG have been released in the last few years, and software package like MNE (CITE) provide us an easy way to wrap them in a common format. It is now time for the field of BCI create the tools that will help grow the next generation of BCI algorithms. (Meh ...)

We propose our platform, the MOABB
(Mother Of All BCI Benchmarks) Project, as a candidate for this application. 

As an initial validation of this project, we present results on the constrained
task of binary classification in left or right hand imagined motor imagery (Why should we restrict to left-right ?), as
that is the most widely used motor imagery paradigm and allows us to exhibit the
process across the largest number of datasets.
However, the format allows for
many other questions, including different channel types (EEG, fNIRS, or other),
multi-class paradigms, and also transfer learning scenarios as described in
\cite{Jayaram2016}. -> is this sentence necessary ?

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
