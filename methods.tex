Any BCI analysis is defined by three things: A dataset, a context, and
a processing pipeline. Here we describe how all of these components
are dealt with within our pipeline, and how specifically we set the
options for the initial analyses presented here.

\subsection{Datasets}

Public BCI datasets exist for a wide range of user paradigms and
recording conditions, from continuous usage to single-session to
multiple-sessions-per-subject. Within the current MOABB project, we
have unified the access to many datasets, described in Table
\ref{fig:datasets}.


\begin{table*}[ht]
  \centering
  \begin{tabular}{l || c | c | c | c | c | c | c }
    Name & Imagery & \# Channels & \# Trials & \# Sessions & \# Subjects & Epoch & Citations \\ \hline
    Cho et al. 2017 & Right, left hand & 64 & 200 & 1 & 49 & 0-3s & \cite{Cho2017} \\
    Physionet & Right, left hand & 64 & 40-60 & 1 & 109 & 1-3s & \cite{Schalk2004, Goldberger2000} \\
    Shin et al. 2017 & Right, left hand & 25 & 60 & 3 & 29 & 0-10s & \cite{Blankertz2007, Shin2017} \\
    BNCI 2014-001 & Right, left hand & 22 & 144 & 2 & 9 & 2-6s & \cite{Tangermann2012} \\
    BNCI 2014-002 & Right hand, feet & 15 & 160 & 1 & 14 & 3-8s & \cite{Steyrl2016} \\
    BNCI 2014-004 & Right, left hand & 3 & 120-160 & 5 & 9 & 3-7.5s & \cite{Leeb2007} \\
    BNCI 2015-001 & Right hand, feet & 13 & 200 & 2/3 & 13 & 3-8s & \cite{Faller2012} \\
    BNCI 2015-004 & Right hand, feet & 30 & 70-80 & 2 & 10 & 3-10s & \cite{Scherer2015a} \\
    Alexandre Motor Imagery & Right hand, feet & 16 & 40 & 1 & 9 & 0-3s & \cite{Barachant2012a}\\
    Yi et al. 2014 & Right, left hand & 60 & 160 & 1 & 10 & 3-7s & \cite{Yi2014} \\
    Zhou et al. 2016 & Right, left hand & 14 & 100 & 3 & 4 & 1-6s & \cite{Zhou2016}\\
    Grosse-Wentrup et al. 2009 & Right, left hand & 128 & 300 & 1 & 10 & 3-10s & \cite{Grosse-Wentrup2009} \\
    \hline
    \centering\bf{Total}: & & & & & \bf{275}& &
    
\end{tabular}
    \caption{Dataset attributes}
    \label{fig:datasets}
\end{table*}

Adding new open-source datasets is also simple. The MNE toolkit
\cite{Gramfort2014,Gramfort2013} is used for all preprocessing and channel
selection, so any dataset that can be made compatible with their framework can
quickly be added to the set of data offered by this project. In addition, the
project offers test functions to ensure candidate code conforms to the software
interface.

\subsection{Context}

A \emph{context} is the set of characteristics that defines the
preprocessing and validation procedure. To go from a recorded EEG
time-series to a pipeline performance value for a given subject or
recording session, many parameters must be defined. First, trials need to be
cut out of the continuous signal and pre-processed, which is
possible in many different ways when taking into account parameters such as
trial overlap, trial length, imagery type, and more. Once the
continuous data is processed into trials, and these trials are fed
into a pipeline, the next question of how to create training and test sets,
and how to report performance, comes into play. We separate these two
notions in our software and call them the \emph{paradigm} and the
\emph{evaluation} respectively.

\subsubsection{Paradigm}

A paradigm defines how one goes from continuous data to trials for a standard
machine learning pipeline to deal with. While not an issue in image processing,
as each trial is just one image, it is crucial in EEG and biosignals processing
because most datasets do not have exactly the same events defined in the
continuous data. For example, many datasets with two-class motor imagery use
left versus right hand, while some use hands versus feet; there are also many
possible non-motor imageries. For any reasonable analysis the specific sort of
imagery or ERP must be controlled for, as they all have different
characteristics in the data and further are variably effective across subjects
\cite{Scherer2015a,Allison2010}. After choosing which events or imageries are
valid, the question comes to pre-processing of the continuous data, in the form
of ICA cleaning, bandpass filtering, and so on. These must also be identical for
valid comparisons across algorithm or datasets. Lastly, there are questions of
how to cut the data into trials: What is the trial length and overlap; or, in
the case of ERP paradigms, how long before and after the event marker do we use?
The answers to all these questions are summed up in the paradigm object.

\subsubsection{Evaluation}

Once the data is split into trials and a pipeline is fixed, there are many ways
to train and test this pipeline to minimize overfitting. For datasets with
multiple subjects recorded on multiple days, we may want to determine which
algorithm functions best in multi-day classification. Or, we may want to
determine which algorithm is best for small amounts of training data. It is easy
to see that there are many possibilities for splitting data into train and test
sets depending on the question to be answered, and these must be fixed
identically for a given analysis. Furthermore, there is the question of how to
report results. Multiclass problems cannot use metrics like the ROC-AUC which
provide unbiased estimates of classifier goodness in binary cases; depending on
things like the class balance, various other metrics have their own benefits and
pitfalls. Therefore this must also be fixed across all datasets, contingent on
the class of predictions the pipelines attempt to make. We define this as our
\emph{evaluation}.

\subsection{Pipeline}

We define a \emph{pipeline} as the processing that takes one from raw
trial-wise data into labels, taking both spatial filtering and
classification model fitting into account. A convenient API for
dealing with this kind of processing is defined by
scikit-learn \cite{Pedregosa2011}, which allows for easily definable
dimensionality reduction, feature generation, and model fitting. To
maximize reproducibility we allow pipelines to be defined either by
yaml files or through python files that generate the objects, but
force all machine learning models to follow the scikit-learn
interface. 

\section{Statistical Analysis}
\label{stats}

At the end of processing there are scores for every subject in every dataset
with every pipeline. The goal of this project is to synthesize these numbers
into an estimate of how likely it is that each pipeline out-performs the other
pipelines. However, even if imagery type and channel number were held constant,
differences in trial amount, sampling rate, and even location and hardware mean
that we cannot expect subjects across datasets to be naively comparable. Simply
pooling them all and running a paired-sample test would result in misleading
significances due to these factors. This would suggest a mixed-factor ANOVA for
every unique pair of pipelines to test the null hypothesis that the difference
distribution is zero-mean over all datasets. However, we have the secondary
problem that this difference distribution, even within a given dataset, is very
unlikely to be Gaussian (which is an assumption of an ANOVA). It is well-known
that some subjects are BCI illiterate\cite{Allison2010}, which implies that no
pipeline can reliably out-predict another one on that subset of
subjects. Therefore, for large enough datasets, the distribution of differences
in pipeline scores is very likely to be at least bimodal.

To deal with this issue while also keeping the framework running fast enough to
execute on a normal desktop, we use a mixture of permutation and non-parametric
tests. Within each dataset, either a one-tailed permutation-based paired t-test
or Wilcoxon sign-rank test is run (depending on the number of subjects) for each
pair of pipelines, generating a p-value for the hypothesis that pipeline $a$ is
bigger than pipeline $b$ for each pair of pipelines. These p-values are combined
via Stouffer's method, with a weighting given by the square root of the number
of subjects, to return a final p-value for each hypothesis. Since each score is
compared against $N_{pipelines}-1$ other scores for the same subject, we also
apply Bonferroni correction to protect against false positives. In order to
determine effect size, we computed the standardized mean difference within
datasets and combined them using the same weighting as was given to Stouffer's
method. 

\section{Experiment}

To show off the possibilities of this framework, we ran various well-known BCI
pipelines from across many papers in order to conduct the first big-data,
side-by-side analysis of the state of the art in motor imagery BCIs.

\subsection{Context}
For the paradigm, we choose to look at datasets including motor
imagery.  Motor imagery is the most-studied sort of imagery for BCIs
\cite{Yuan2014}, and we further limit ourselves to the binary case as
this has not yet been solved. For evaluations, we choose
within-session cross-validation, as this represents the best-case
scenario for any pipeline, with minimal non-stationarity. 

\subsubsection{Paradigm}
% As there are many methods that show that multiple frequency bands can
% lead to improved BCI performance\cite{KaiKengAng2008}, and further
% that discriminative data is concentrated in the anatomical frequency
% bands, we test three preprocessing pipelines: A single bandpass
% containing both the alpha and beta ranges, from $8-32\text{Hz}$,
% another from $8-32\text{Hz}$ in 4Hz increments, and a third with the
% $\delta, \theta, \alpha$ and $\beta$ frequencies isolated.

As there are many methods that show that multiple frequency bands can lead to
improved BCI performance\cite{KaiKengAng2008}, and further that discriminative
data is concentrated in the anatomical frequency bands, we test two
preprocessing pipelines: A single bandpass containing both the alpha and beta
ranges, from $8-32\text{Hz}$, and another from $8-32\text{Hz}$ in 4Hz
increments. All data was also subsampled to 128Hz, as the memory requirements
became prohibitive otherwise.

\subsubsection{Evaluation}
The evaluation was chosen to be within-session, as that minimizes the effect of
non-stationarity. As this is a binary classification task, the ROC-AUC score was
chosen as the metric to score 5-fold cross validation (the splits were kept
identical for all pipelines in a given subject). In comparison with the more
interpretable classification accuracy, the ROC-AUC is less sensitive to
imbalanced classes, which is important in this case where the datasets vary
heavily. In order to return a single score per subject, the scores from each
session were averaged when multiple sessions were present.

\subsection{Pipelines}

Given the sheer breadth of models implemented within scikit-learn,
attempting an exhaustive model comparison would be impossible. Instead, we
implement a selection of pipelines from the BCI literature, as well as the
well-known standards of CSP + LDA and channel-level variances + SVM. Specific
implemented pipelines are in Table \ref{tab:pipelines}; all hyperparameters were set via cross-validation.
\begin{table*}
  \centering
  \begin{tabular}{ l || p{6cm} | p{6cm} | c | }
    
    Name & Preprocessing & Classifier \\ \hline
    CSP + LDA & Trial covariances estimated via maximum-likelihood with unregularized common spatial patterns (CSP). Features were log variance of the filters belonging to the 6 most diverging eigenvalues & Linear Discriminant Analysis (LDA) & \cite{Koles1990} \\ \hline
    RegCSP + shLDA & Trial covariances estimated by OAS (Chen) with
                     unregularized CSP. This is equivalent to Tikhonov
                     regularization as described in \cite{Lotte2011}. Features were log variance on the 6 top filters. & LDA with Ledoit-Wolf shrinkage of the covariance term  & \cite{Lotte2011} \\ \hline
    rieCSP + shLDA & Trial covariances estimated via maximum-likelihood, CSP class-wise matrices were Riemannian mean of the trial-wise matrices. & LDA with Ledoit-Wolf shrinkage of the covariance term  & \cite{Barachant2010a} \\ \hline
    FBCSP + optSVM & Filter bank of 6 bands between 8 and 35 Hz followed by OAS covariance estimation and unregularized CSP. Log variance from each of the 4 top filters from each sub-band were pooled and the top 10 features chosen by mutual information were used. & A linear support vector machine was trained with its regularization hyperparameter set by a cross-validated grid-search from $[0.01 100]$. & \cite{KaiKengAng2008} \\ \hline
    TS + optSVM & Trial covariances estimated via OAS then projected into the Riemannian tangent space to obtain features & Linear SVM with identical grid-search & \cite{Barachant2013} \\ \hline
    AM + optSVM & Log variance in each channel & Linear SVM with grid-search & N/A \\ \hline
    % FB-AM + optLR & Log variance from each channel in the five anatomical frequency bands ($\delta, \theta, \alpha, \beta, \gamma$) & To replicate the genetic algorithm and channel selection used in the paper, as well as the linear final boundary, we use a logistic regression classifier with an L1 penalty, hyperparameter optimized via grid search& \cite{Garrett2003} \\
    
  \end{tabular}
  \caption{Processing pipelines}
  \label{tab:pipelines}
\end{table*}
  

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main.tex"
%%% End:
